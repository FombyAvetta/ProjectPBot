# OpenClaw Environment Configuration
# Copy this file to .env and fill in your values

# ============================================
# Gateway Configuration
# ============================================
GATEWAY_PORT=18789
GATEWAY_HOST=0.0.0.0

# ============================================
# Data Persistence
# ============================================
DATA_DIR=./data
LOGS_DIR=./logs

# ============================================
# Claude API Configuration (Primary LLM)
# ============================================
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-your-key-here

# ============================================
# LLM Provider Configuration
# ============================================
# Options: anthropic, openai, ollama, qwen3
# - anthropic: Claude API (cloud, requires API key)
# - openai: OpenAI API (cloud, requires API key)
# - ollama: Local Ollama server
# - qwen3: Local Qwen3 4B (offline, see Qwen3 configuration below)
LLM_PROVIDER=anthropic

# ============================================
# OpenAI Configuration (Alternative LLM)
# ============================================
# Get your API key from: https://platform.openai.com/
OPENAI_API_KEY=

# ============================================
# Ollama Configuration (Local LLM)
# ============================================
# For running local models on Jetson
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama2

# ============================================
# Qwen3 Local LLM Configuration
# ============================================
# Qwen3 4B provides local, offline LLM inference on Jetson Nano
# - Completely offline operation (no internet required)
# - Free after initial setup
# - 10-15 tokens/sec generation speed
# - ~2.5GB model size, ~3-4GB memory usage when running
#
# Options: qwen3 (add to LLM_PROVIDER to enable)
# Setup: ./scripts/05-qwen3-setup.sh
# Enable: Set QWEN3_ENABLED=true and start with --profile qwen3

# Enable/disable Qwen3 service (default: false)
QWEN3_ENABLED=false

# Model file path (download via setup script)
QWEN3_MODEL_PATH=/models/Qwen3-4B-Q4_K_M.gguf

# API endpoint (internal Docker network)
QWEN3_HOST=http://qwen3-server:8080

# Performance Settings
# --------------------
# Context Length: Maximum number of tokens in conversation context
# - 1024: Minimal memory (~2.2GB), faster, good for simple tasks
# - 2048: Balanced memory (~2.8GB), recommended for most use cases
# - 4096: High memory (~4.5GB), slower, best for long conversations
# Default: 2048 (balanced)
QWEN3_CONTEXT_LENGTH=2048

# GPU Layers: Number of model layers offloaded to GPU
# - 0:    CPU-only mode (~2-3 tok/s, lowest memory)
# - 24:   Partial GPU offload (~8-10 tok/s, ~2.2GB)
# - 32:   Balanced GPU offload (~10-12 tok/s, ~2.8GB) [RECOMMENDED]
# - 99:   Full GPU offload (~12-15 tok/s, ~3.5GB)
# - auto: Automatically detect optimal based on available GPU memory
# Default: 32 (balanced)
QWEN3_GPU_LAYERS=32

# Batch Size: Number of tokens processed in parallel
# - 256:  Lower memory, slightly slower
# - 512:  Balanced performance [RECOMMENDED]
# - 1024: Higher throughput, more memory
# Default: 512
QWEN3_BATCH_SIZE=512

# Threads: Number of CPU threads for computation
# - 2-4: Recommended for Jetson Nano (leave headroom for other services)
# - 6:   Maximum for Jetson Nano (may impact other services)
# Default: 4
QWEN3_THREADS=4

# Parallel Requests: Number of simultaneous inference requests
# - 1: Sequential processing only [RECOMMENDED for 8GB]
# - 2: Concurrent requests (requires more memory)
# Default: 1
QWEN3_PARALLEL_REQUESTS=1

# Request Timeout: Maximum time for inference request (seconds)
# Default: 600 (10 minutes)
QWEN3_TIMEOUT=600

# Memory Limit: Docker container memory limit
# - 5g: Conservative, safe for 8GB Jetson [RECOMMENDED]
# - 6g: Aggressive, may cause OOM if other services running
# Default: 5g
QWEN3_MEMORY_LIMIT=5g

# Recommended Configuration Profiles
# -----------------------------------
# Profile 1: Minimal Memory (Stable)
#   QWEN3_CONTEXT_LENGTH=1024
#   QWEN3_GPU_LAYERS=24
#   QWEN3_BATCH_SIZE=256
#   Expected: ~2.2GB memory, ~8-10 tok/s
#
# Profile 2: Balanced (Recommended)
#   QWEN3_CONTEXT_LENGTH=2048
#   QWEN3_GPU_LAYERS=32
#   QWEN3_BATCH_SIZE=512
#   Expected: ~2.8GB memory, ~10-12 tok/s
#
# Profile 3: Maximum Performance (Risky on 8GB)
#   QWEN3_CONTEXT_LENGTH=4096
#   QWEN3_GPU_LAYERS=99
#   QWEN3_BATCH_SIZE=1024
#   Expected: ~4.5GB memory, ~12-15 tok/s
#
# Use Profile 2 (Balanced) for best reliability on 8GB Jetson Nano

# ============================================
# Telegram Bot Configuration
# ============================================
# Create a bot via @BotFather on Telegram
# Tutorial: https://core.telegram.org/bots/tutorial
TELEGRAM_BOT_TOKEN=
TELEGRAM_ENABLED=false

# ============================================
# Discord Bot Configuration
# ============================================
# Create a bot at: https://discord.com/developers/applications
# Enable MESSAGE CONTENT INTENT in bot settings
DISCORD_BOT_TOKEN=
DISCORD_ENABLED=false

# ============================================
# WhatsApp Configuration (Advanced)
# ============================================
WHATSAPP_ENABLED=false
WHATSAPP_PHONE_NUMBER=
WHATSAPP_API_KEY=

# ============================================
# Signal Configuration (Advanced)
# ============================================
SIGNAL_ENABLED=false
SIGNAL_PHONE_NUMBER=

# ============================================
# Security
# ============================================
# Set a strong admin password
ADMIN_PASSWORD=change-this-password

# JWT secret for API authentication
JWT_SECRET=your-random-secret-key-here

# ============================================
# Logging
# ============================================
# Options: debug, info, warning, error, critical
LOG_LEVEL=info

# ============================================
# NVIDIA/CUDA Configuration (Jetson)
# ============================================
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility

# ============================================
# Advanced Settings
# ============================================
# Maximum conversation history length
MAX_HISTORY_LENGTH=50

# Request timeout in seconds
REQUEST_TIMEOUT=30

# Enable debug mode
DEBUG=false

# Database configuration
DATABASE_URL=sqlite:///data/openclaw.db

# Redis configuration (optional, for scaling)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_ENABLED=false
